{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COPERNICUS Vegetation Indicatorss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"service\">Service Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = dict([('title', 'COPERNICUS Vegetation Indicators - Aggregations'),\n",
    "                ('abstract', 'COPERNICUS Vegetation Indicators - Aggregations'),\n",
    "                ('id', 'wfp-01-03-01')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"parameter\">Parameter Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_1 = dict([('id', 'N_1'),\n",
    "                          ('value', 'True'),\n",
    "                          ('title', 'No Aggregation'),\n",
    "                          ('abstract', 'No aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_3 = dict([('id', 'N_3'),\n",
    "                          ('value', 'True'),\n",
    "                          ('title', '30 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 30 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_6 = dict([('id', 'N_6'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', '60 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 60 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_9 = dict([('id', 'N_9'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', '90 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 90 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_12 = dict([('id', 'N_12'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', '120 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 120 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_15 = dict([('id', 'N_15'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', '150 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 150 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_18 = dict([('id', 'N_18'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', '180 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 180 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_27 = dict([('id', 'N_27'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', '270 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 270 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_36 = dict([('id', 'N_36'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', '360 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 360 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionOfInterest = dict([('id', 'regionOfInterest'),\n",
    "                          ('value', 'POLYGON((11.5030755518998 -11.1141633706909,41.0343255518998 -11.1141633706909,41.0343255518998 -34.9763656693858,11.5030755518998 -34.9763656693858,11.5030755518998 -11.1141633706909))'),\n",
    "                          ('title', 'WKT Polygon for the Region of Interest'),\n",
    "                          ('abstract', 'Set the value of WKT Polygon')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameOfRegion = dict([('id', 'nameOfRegion'),\n",
    "                     ('value', ''),\n",
    "                     ('title', 'Name of Region'),\n",
    "                     ('abstract', 'Name of the region of interest'),\n",
    "                     ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lta_url = dict([('id', 'lta_url'),\n",
    "                ('value', 'https://catalog.terradue.com//better-wfp-00009/series/results/search'),\n",
    "                ('title', 'Catalogue Url for the LTA products'),\n",
    "                ('abstract', 'Catalogue Url for anomalies')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"runtime\">Runtime parameter definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input references**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "param": "aoi"
   },
   "outputs": [],
   "source": [
    "input_references = ('https://catalog.terradue.com/cgls/search?format=atom&uid=fapar_v2_1km_FAPAR-RT6_201702280000_GLOBE_PROBAV_V2.0.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"workflow\">Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the packages required for processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#sys.path.append('/opt/OTB-6.2.0/lib/python')\n",
    "#sys.path.append('/opt/OTB-6.2.0/lib/libfftw3.so.3')\n",
    "#os.environ['OTB_APPLICATION_PATH'] = '/opt/OTB-6.2.0/lib/otb/applications'\n",
    "#os.environ['LD_LIBRARY_PATH'] = '/opt/OTB-6.2.0/lib'\n",
    "#os.environ['ITK_AUTOLOAD_PATH'] = '/opt/OTB-6.2.0/lib/otb/applications'\n",
    "#os.environ['GDAL_DATA'] = '/opt/anaconda/share/gdal/'\n",
    "#import otbApplication\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopandas import GeoDataFrame\n",
    "from datetime import datetime\n",
    "import math\n",
    "import re\n",
    "import cioppy\n",
    "import shutil\n",
    "import requests\n",
    "from shapely.wkt import loads\n",
    "sys.path.append('/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec')\n",
    "sys.path.append('/application/notebook/libexec')\n",
    "from aux_functions import calc_average, calc_max_matrix, matrix_sum, crop_image, write_output_image, get_matrix_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(row, search_params):\n",
    "    search = ciop.search(end_point=row['catalogue_url'], \n",
    "                                  params=search_params,\n",
    "                                  output_fields='self,identifier,startdate,enclosure',\n",
    "                                  model='GeoTime')[0]\n",
    "    \n",
    "    series = pd.Series(search)\n",
    "    \n",
    "    series['startdate'] = pd.to_datetime(series['startdate'])\n",
    "    \n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lta_list(url, search_params):\n",
    "    search = ciop.search(end_point=url,\n",
    "                         params=search_params,\n",
    "                         output_fields='self,identifier,enclosure,title,startdate,enddate,wkt,updated',\n",
    "                         model='GeoTime')\n",
    "    return search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    https://catalog.terradue.com/cgls/search?forma...\n",
      "1    https://catalog.terradue.com/cgls/search?forma...\n",
      "0    https://catalog.terradue.com/cgls/search?forma...\n",
      "5    https://catalog.terradue.com/cgls/search?forma...\n",
      "4    https://catalog.terradue.com/cgls/search?forma...\n",
      "3    https://catalog.terradue.com/cgls/search?forma...\n",
      "Name: catalogue_url, dtype: object\n",
      "2017-01-10\n",
      "2017-02-28\n",
      "6\n",
      "5    https://catalog.terradue.com/cgls/search?forma...\n",
      "4    https://catalog.terradue.com/cgls/search?forma...\n",
      "3    https://catalog.terradue.com/cgls/search?forma...\n",
      "2    https://catalog.terradue.com/cgls/search?forma...\n",
      "1    https://catalog.terradue.com/cgls/search?forma...\n",
      "0    https://catalog.terradue.com/cgls/search?forma...\n",
      "Name: catalogue_url, dtype: object\n",
      "2017-01-10\n",
      "2017-02-28\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "if isinstance(input_references, str):\n",
    "    input_references = [input_references]\n",
    "\n",
    "\n",
    "region_of_interest = regionOfInterest['value']\n",
    "nameOfRegion = nameOfRegion['value']\n",
    "#nameOfRegion = 'SouthernAfrica'\n",
    "ciop = cioppy.Cioppy()\n",
    "\n",
    "lai_references = [ref for ref in input_references if 'LAI' in ref]\n",
    "fapar_references = [ref for ref in input_references if 'FAPAR' in ref]\n",
    "gpd_lai_final = None\n",
    "gpd_fapar_final = None\n",
    "if lai_references:\n",
    "    gpd_data = GeoDataFrame(lai_references,\n",
    "                           columns=['catalogue_url'])\n",
    "\n",
    "    gpd_data = gpd_data.sort_values(by='catalogue_url')\n",
    "    print(gpd_data['catalogue_url'])\n",
    "    start_date = re.findall('\\d{8}0000', gpd_data.iloc[0]['catalogue_url'])[0].replace('0000', '')\n",
    "    end_date = re.findall('\\d{8}0000', gpd_data.iloc[-1]['catalogue_url'])[0].replace('0000', '')\n",
    "    start_date = re.sub(r'(\\d{4})(\\d{2})(\\d{2})', r'\\1-\\2-\\3', start_date)\n",
    "    end_date = re.sub(r'(\\d{4})(\\d{2})(\\d{2})', r'\\1-\\2-\\3', end_date)\n",
    "    print(start_date)\n",
    "    print(end_date)\n",
    "    print(len(lai_references))\n",
    "    search_params =  dict([('start', start_date),\n",
    "                          ('stop', end_date),\n",
    "                          ('count', len(lai_references))])\n",
    "\n",
    "    gpd_lai_final = gpd_data.apply(lambda row: get_info(row, search_params), axis=1)\n",
    "\n",
    "if fapar_references:\n",
    "    gpd_data = GeoDataFrame(fapar_references,\n",
    "                           columns=['catalogue_url'])\n",
    "\n",
    "    gpd_data = gpd_data.sort_values(by='catalogue_url')\n",
    "    print(gpd_data['catalogue_url'])\n",
    "    start_date = re.findall('\\d{8}0000', gpd_data.iloc[0]['catalogue_url'])[0].replace('0000', '')\n",
    "    end_date = re.findall('\\d{8}0000', gpd_data.iloc[-1]['catalogue_url'])[0].replace('0000', '')\n",
    "    start_date = re.sub(r'(\\d{4})(\\d{2})(\\d{2})', r'\\1-\\2-\\3', start_date)\n",
    "    end_date = re.sub(r'(\\d{4})(\\d{2})(\\d{2})', r'\\1-\\2-\\3', end_date)\n",
    "    print(start_date)\n",
    "    print(end_date)\n",
    "    print(len(fapar_references))\n",
    "    search_params =  dict([('start', start_date),\n",
    "                          ('stop', end_date),\n",
    "                          ('count', len(fapar_references))])\n",
    "\n",
    "    gpd_fapar_final = gpd_data.apply(lambda row: get_info(row, search_params), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 150, 'start': '2017-01-10T00:00:00.0000000Z', 'geom': 'POLYGON((11.5030755518998 -11.1141633706909,41.0343255518998 -11.1141633706909,41.0343255518998 -34.9763656693858,11.5030755518998 -34.9763656693858,11.5030755518998 -11.1141633706909))', 'stop': '2017-02-28T00:00:00.0000000Z'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enclosure</th>\n",
       "      <th>enddate</th>\n",
       "      <th>identifier</th>\n",
       "      <th>self</th>\n",
       "      <th>startdate</th>\n",
       "      <th>title</th>\n",
       "      <th>updated</th>\n",
       "      <th>wkt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://store.terradue.com/better-wfp-00009/_r...</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>01ED57858120D350BCD467224037F18C964F6CD5</td>\n",
       "      <td>https://catalog.terradue.com//better-wfp-00009...</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>Output LTA_CHIRPSv2_BotswanaZambia_N30_daystot...</td>\n",
       "      <td>2019-03-11T15:39:44.3101830+00:00</td>\n",
       "      <td>POLYGON ((19.5641183795274 -14.9105384014693, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://store.terradue.com/better-wfp-00009/_r...</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>0C12098705F6C606B0A946D85A7A39770D5D461F</td>\n",
       "      <td>https://catalog.terradue.com//better-wfp-00009...</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Output LTA_CHIRPSv2_SouthernAfrica_N30_dryspel...</td>\n",
       "      <td>2019-03-11T15:39:44.3101810+00:00</td>\n",
       "      <td>POLYGON ((11.5030755518998 -11.1141633706909, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://store.terradue.com/better-wfp-00009/_r...</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>147931514ADF3BD9A3EB3C72F097224304197E8C</td>\n",
       "      <td>https://catalog.terradue.com//better-wfp-00009...</td>\n",
       "      <td>2015-01-10</td>\n",
       "      <td>Output LTA_LAI_SouthernAfrica_N3_averages_1-10...</td>\n",
       "      <td>2019-03-11T16:09:09.0879070+00:00</td>\n",
       "      <td>POLYGON ((11.5030755518998 -11.1141633706909, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://store.terradue.com/better-wfp-00009/_r...</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>55813C2ADE9BD1824A03AF21CE7740A22A8CD0BF</td>\n",
       "      <td>https://catalog.terradue.com//better-wfp-00009...</td>\n",
       "      <td>2015-01-10</td>\n",
       "      <td>Output LTA_FAPAR_SouthernAfrica_N3_averages_1-...</td>\n",
       "      <td>2019-03-11T16:09:09.0879080+00:00</td>\n",
       "      <td>POLYGON ((11.5030755518998 -11.1141633706909, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://store.terradue.com/better-wfp-00009/_r...</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>6AD86AB8988A776CB2939FBEF5AD36B52050B099</td>\n",
       "      <td>https://catalog.terradue.com//better-wfp-00009...</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>Output LTA_CHIRPSv2_BotswanaZambia_N30_countab...</td>\n",
       "      <td>2019-03-11T15:39:44.3101830+00:00</td>\n",
       "      <td>POLYGON ((19.5641183795274 -14.9105384014693, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://store.terradue.com/better-wfp-00009/_r...</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>7558809279D798348CD475AACAA52869D963A76E</td>\n",
       "      <td>https://catalog.terradue.com//better-wfp-00009...</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Output LTA_CHIRPSv2_SouthernAfrica_N30_daystot...</td>\n",
       "      <td>2019-03-11T15:39:44.3101810+00:00</td>\n",
       "      <td>POLYGON ((11.5030755518998 -11.1141633706909, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://store.terradue.com/better-wfp-00009/_r...</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>86A0BE56B5DFBDE03197DCE0E3491F36BDA769A2</td>\n",
       "      <td>https://catalog.terradue.com//better-wfp-00009...</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Output LTA_CHIRPSv2_SouthernAfrica_N30_countab...</td>\n",
       "      <td>2019-03-11T15:39:44.3101820+00:00</td>\n",
       "      <td>POLYGON ((11.5030755518998 -11.1141633706909, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://store.terradue.com/better-wfp-00009/_r...</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>9703BEB32893CE6719C905914CC52E349846F568</td>\n",
       "      <td>https://catalog.terradue.com//better-wfp-00009...</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>Output LTA_CHIRPSv2_BotswanaZambia_N30_dryspel...</td>\n",
       "      <td>2019-03-11T15:39:44.3101820+00:00</td>\n",
       "      <td>POLYGON ((19.5641183795274 -14.9105384014693, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://store.terradue.com/better-wfp-00009/_r...</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>9BF5187EBF0CE31AEB91766552F8AB76F7C0B690</td>\n",
       "      <td>https://catalog.terradue.com//better-wfp-00009...</td>\n",
       "      <td>2015-01-10</td>\n",
       "      <td>Output LTA_LAI_SouthernAfrica_N3_maxvalues_1-1...</td>\n",
       "      <td>2019-03-11T16:09:09.0879070+00:00</td>\n",
       "      <td>POLYGON ((11.5030755518998 -11.1141633706909, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://store.terradue.com/better-wfp-00009/_r...</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>F4DE56825DB1A70F233AE41C21F453280DB0B8E9</td>\n",
       "      <td>https://catalog.terradue.com//better-wfp-00009...</td>\n",
       "      <td>2015-01-10</td>\n",
       "      <td>Output LTA_FAPAR_SouthernAfrica_N3_maxvalues_1...</td>\n",
       "      <td>2019-03-11T16:09:09.0879080+00:00</td>\n",
       "      <td>POLYGON ((11.5030755518998 -11.1141633706909, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://store.terradue.com/better-wfp-00009/_r...</td>\n",
       "      <td>2017-01-10</td>\n",
       "      <td>1036A9BE825E14EA3ECADB5B0298537ED59B9134</td>\n",
       "      <td>https://catalog.terradue.com//better-wfp-00009...</td>\n",
       "      <td>2015-01-10</td>\n",
       "      <td>Output LTA_LAI_SouthernAfrica_N3_maxvalues_1-1...</td>\n",
       "      <td>2019-03-08T16:19:09.2599870+00:00</td>\n",
       "      <td>POLYGON ((11.5030755518998 -11.1141633706909, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://store.terradue.com/better-wfp-00009/_r...</td>\n",
       "      <td>2017-01-10</td>\n",
       "      <td>150A971BED4D23C6A2405E54B44B99FC3AF7A386</td>\n",
       "      <td>https://catalog.terradue.com//better-wfp-00009...</td>\n",
       "      <td>2015-01-10</td>\n",
       "      <td>Output LTA_FAPAR_SouthernAfrica_N3_averages_1-...</td>\n",
       "      <td>2019-03-08T16:19:09.2599880+00:00</td>\n",
       "      <td>POLYGON ((11.5030755518998 -11.1141633706909, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://store.terradue.com/better-wfp-00009/_r...</td>\n",
       "      <td>2017-01-10</td>\n",
       "      <td>636448E3A888FA320A99809B1C101BE0ABB1FC2D</td>\n",
       "      <td>https://catalog.terradue.com//better-wfp-00009...</td>\n",
       "      <td>2015-01-10</td>\n",
       "      <td>Output LTA_LAI_SouthernAfrica_N3_averages_1-10...</td>\n",
       "      <td>2019-03-08T16:19:09.2599870+00:00</td>\n",
       "      <td>POLYGON ((11.5030755518998 -11.1141633706909, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://store.terradue.com/better-wfp-00009/_r...</td>\n",
       "      <td>2017-01-10</td>\n",
       "      <td>797B75D777387DD2BEC098DD7C22CBC5D8E054F3</td>\n",
       "      <td>https://catalog.terradue.com//better-wfp-00009...</td>\n",
       "      <td>2015-01-10</td>\n",
       "      <td>Output LTA_FAPAR_SouthernAfrica_N3_maxvalues_1...</td>\n",
       "      <td>2019-03-08T16:19:09.2599880+00:00</td>\n",
       "      <td>POLYGON ((11.5030755518998 -11.1141633706909, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            enclosure    enddate  \\\n",
       "0   https://store.terradue.com/better-wfp-00009/_r... 2017-01-31   \n",
       "1   https://store.terradue.com/better-wfp-00009/_r... 2017-01-31   \n",
       "2   https://store.terradue.com/better-wfp-00009/_r... 2017-01-31   \n",
       "3   https://store.terradue.com/better-wfp-00009/_r... 2017-01-31   \n",
       "4   https://store.terradue.com/better-wfp-00009/_r... 2017-01-31   \n",
       "5   https://store.terradue.com/better-wfp-00009/_r... 2017-01-31   \n",
       "6   https://store.terradue.com/better-wfp-00009/_r... 2017-01-31   \n",
       "7   https://store.terradue.com/better-wfp-00009/_r... 2017-01-31   \n",
       "8   https://store.terradue.com/better-wfp-00009/_r... 2017-01-31   \n",
       "9   https://store.terradue.com/better-wfp-00009/_r... 2017-01-31   \n",
       "10  https://store.terradue.com/better-wfp-00009/_r... 2017-01-10   \n",
       "11  https://store.terradue.com/better-wfp-00009/_r... 2017-01-10   \n",
       "12  https://store.terradue.com/better-wfp-00009/_r... 2017-01-10   \n",
       "13  https://store.terradue.com/better-wfp-00009/_r... 2017-01-10   \n",
       "\n",
       "                                  identifier  \\\n",
       "0   01ED57858120D350BCD467224037F18C964F6CD5   \n",
       "1   0C12098705F6C606B0A946D85A7A39770D5D461F   \n",
       "2   147931514ADF3BD9A3EB3C72F097224304197E8C   \n",
       "3   55813C2ADE9BD1824A03AF21CE7740A22A8CD0BF   \n",
       "4   6AD86AB8988A776CB2939FBEF5AD36B52050B099   \n",
       "5   7558809279D798348CD475AACAA52869D963A76E   \n",
       "6   86A0BE56B5DFBDE03197DCE0E3491F36BDA769A2   \n",
       "7   9703BEB32893CE6719C905914CC52E349846F568   \n",
       "8   9BF5187EBF0CE31AEB91766552F8AB76F7C0B690   \n",
       "9   F4DE56825DB1A70F233AE41C21F453280DB0B8E9   \n",
       "10  1036A9BE825E14EA3ECADB5B0298537ED59B9134   \n",
       "11  150A971BED4D23C6A2405E54B44B99FC3AF7A386   \n",
       "12  636448E3A888FA320A99809B1C101BE0ABB1FC2D   \n",
       "13  797B75D777387DD2BEC098DD7C22CBC5D8E054F3   \n",
       "\n",
       "                                                 self  startdate  \\\n",
       "0   https://catalog.terradue.com//better-wfp-00009... 2016-01-01   \n",
       "1   https://catalog.terradue.com//better-wfp-00009... 2015-01-01   \n",
       "2   https://catalog.terradue.com//better-wfp-00009... 2015-01-10   \n",
       "3   https://catalog.terradue.com//better-wfp-00009... 2015-01-10   \n",
       "4   https://catalog.terradue.com//better-wfp-00009... 2016-01-01   \n",
       "5   https://catalog.terradue.com//better-wfp-00009... 2015-01-01   \n",
       "6   https://catalog.terradue.com//better-wfp-00009... 2015-01-01   \n",
       "7   https://catalog.terradue.com//better-wfp-00009... 2016-01-01   \n",
       "8   https://catalog.terradue.com//better-wfp-00009... 2015-01-10   \n",
       "9   https://catalog.terradue.com//better-wfp-00009... 2015-01-10   \n",
       "10  https://catalog.terradue.com//better-wfp-00009... 2015-01-10   \n",
       "11  https://catalog.terradue.com//better-wfp-00009... 2015-01-10   \n",
       "12  https://catalog.terradue.com//better-wfp-00009... 2015-01-10   \n",
       "13  https://catalog.terradue.com//better-wfp-00009... 2015-01-10   \n",
       "\n",
       "                                                title  \\\n",
       "0   Output LTA_CHIRPSv2_BotswanaZambia_N30_daystot...   \n",
       "1   Output LTA_CHIRPSv2_SouthernAfrica_N30_dryspel...   \n",
       "2   Output LTA_LAI_SouthernAfrica_N3_averages_1-10...   \n",
       "3   Output LTA_FAPAR_SouthernAfrica_N3_averages_1-...   \n",
       "4   Output LTA_CHIRPSv2_BotswanaZambia_N30_countab...   \n",
       "5   Output LTA_CHIRPSv2_SouthernAfrica_N30_daystot...   \n",
       "6   Output LTA_CHIRPSv2_SouthernAfrica_N30_countab...   \n",
       "7   Output LTA_CHIRPSv2_BotswanaZambia_N30_dryspel...   \n",
       "8   Output LTA_LAI_SouthernAfrica_N3_maxvalues_1-1...   \n",
       "9   Output LTA_FAPAR_SouthernAfrica_N3_maxvalues_1...   \n",
       "10  Output LTA_LAI_SouthernAfrica_N3_maxvalues_1-1...   \n",
       "11  Output LTA_FAPAR_SouthernAfrica_N3_averages_1-...   \n",
       "12  Output LTA_LAI_SouthernAfrica_N3_averages_1-10...   \n",
       "13  Output LTA_FAPAR_SouthernAfrica_N3_maxvalues_1...   \n",
       "\n",
       "                              updated  \\\n",
       "0   2019-03-11T15:39:44.3101830+00:00   \n",
       "1   2019-03-11T15:39:44.3101810+00:00   \n",
       "2   2019-03-11T16:09:09.0879070+00:00   \n",
       "3   2019-03-11T16:09:09.0879080+00:00   \n",
       "4   2019-03-11T15:39:44.3101830+00:00   \n",
       "5   2019-03-11T15:39:44.3101810+00:00   \n",
       "6   2019-03-11T15:39:44.3101820+00:00   \n",
       "7   2019-03-11T15:39:44.3101820+00:00   \n",
       "8   2019-03-11T16:09:09.0879070+00:00   \n",
       "9   2019-03-11T16:09:09.0879080+00:00   \n",
       "10  2019-03-08T16:19:09.2599870+00:00   \n",
       "11  2019-03-08T16:19:09.2599880+00:00   \n",
       "12  2019-03-08T16:19:09.2599870+00:00   \n",
       "13  2019-03-08T16:19:09.2599880+00:00   \n",
       "\n",
       "                                                  wkt  \n",
       "0   POLYGON ((19.5641183795274 -14.9105384014693, ...  \n",
       "1   POLYGON ((11.5030755518998 -11.1141633706909, ...  \n",
       "2   POLYGON ((11.5030755518998 -11.1141633706909, ...  \n",
       "3   POLYGON ((11.5030755518998 -11.1141633706909, ...  \n",
       "4   POLYGON ((19.5641183795274 -14.9105384014693, ...  \n",
       "5   POLYGON ((11.5030755518998 -11.1141633706909, ...  \n",
       "6   POLYGON ((11.5030755518998 -11.1141633706909, ...  \n",
       "7   POLYGON ((19.5641183795274 -14.9105384014693, ...  \n",
       "8   POLYGON ((11.5030755518998 -11.1141633706909, ...  \n",
       "9   POLYGON ((11.5030755518998 -11.1141633706909, ...  \n",
       "10  POLYGON ((11.5030755518998 -11.1141633706909, ...  \n",
       "11  POLYGON ((11.5030755518998 -11.1141633706909, ...  \n",
       "12  POLYGON ((11.5030755518998 -11.1141633706909, ...  \n",
       "13  POLYGON ((11.5030755518998 -11.1141633706909, ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lta_params = dict([('start', start_date + 'T00:00:00.0000000Z'),\n",
    "                   ('stop', end_date + 'T00:00:00.0000000Z'),\n",
    "                   ('geom', region_of_interest),\n",
    "                   ('count', 150)])\n",
    "print(lta_params)\n",
    "lta_data = get_lta_list(lta_url['value'], lta_params)\n",
    "#lta_data = GeoDataFrame(lta_list, columns=['catalogue_url'])\n",
    "#lta_data = lta_data.apply(lambda row: get_lta_info(row,[]), axis=1)\n",
    "\n",
    "lta_data = GeoDataFrame.from_dict(lta_data)\n",
    "lta_data['startdate'] = pd.to_datetime(lta_data['startdate'])\n",
    "lta_data['enddate'] = pd.to_datetime(lta_data['enddate'])\n",
    "lta_data['wkt'] = lta_data['wkt'].apply(lambda row: loads(row))\n",
    "lta_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lta_from_dataframe(dataframe, region_name, product_type, aggregation, start_day, end_day, start_month, end_month, wkt):\n",
    "    region_polygon = loads(wkt)\n",
    "    dataframe_agr = dataframe[(dataframe['title'].str.contains(aggregation)) &\n",
    "                              (dataframe['title'].str.contains(product_type)) &\n",
    "                              (dataframe['title'].str.contains(region_name))]\n",
    "    period_lta = dataframe_agr[(dataframe_agr['startdate'].dt.day == start_day) & \n",
    "                     (dataframe_agr['startdate'].dt.month == start_month) & \n",
    "                     (dataframe_agr['enddate'].dt.day == end_day) & \n",
    "                     (dataframe_agr['enddate'].dt.month == end_month) &\n",
    "                     (dataframe_agr['wkt'] == region_polygon)]\n",
    "    \n",
    "    if len(period_lta.index.values) > 1:\n",
    "        outdated_indexes = period_lta[period_lta['updated'] != max(period_lta['updated'])].index.values\n",
    "        period_lta = period_lta.drop(outdated_indexes)\n",
    "    return period_lta['enclosure'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product(url, dest):\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    \n",
    "    open(dest, 'wb').write(r.content)\n",
    "    \n",
    "    return r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_aggregations(product_list, N_value, region_of_interest, product_type):\n",
    "    mask_no_data_value = 0\n",
    "    max_values = 0\n",
    "    averages = 0\n",
    "    temp_list = []\n",
    "    no_data_value = None\n",
    "    print(type(product_list))\n",
    "    for product_url in product_list:\n",
    "        # uncompressed data\n",
    "        product = (product_url.split('/')[-1]).split('.gz')[0]\n",
    "        print(product)\n",
    "        cropped_product_path = 'crop_' + product\n",
    "        cropped_product_path = cropped_product_path.split('.nc')[0] + '.tif'\n",
    "        try:\n",
    "            crop_image(product_url, region_of_interest, cropped_product_path, product_type)\n",
    "            # Read GeoTIFF as an array\n",
    "            dataset = gdal.Open(cropped_product_path)\n",
    "            product_array = dataset.GetRasterBand(1).ReadAsArray()\n",
    "            no_data_value = dataset.GetRasterBand(1).GetNoDataValue()\n",
    "            print(no_data_value)\n",
    "            geo_transform = dataset.GetGeoTransform()\n",
    "            projection = dataset.GetProjection()\n",
    "            ## Create mask of no_data_values\n",
    "            if no_data_value is not None:\n",
    "                if isinstance(mask_no_data_value, int):\n",
    "                    mask_no_data_value = np.where(product_array == no_data_value, 1, 0)\n",
    "                else:\n",
    "                    temp_mask = np.where(product_array == no_data_value, 1, 0)\n",
    "                    mask_no_data_value = matrix_sum(mask_no_data_value, temp_mask)\n",
    "            max_values = calc_max_matrix(max_values, product_array, no_data_value)\n",
    "            temp_list.append(product_array)\n",
    "            dataset = None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print('Error processing the product ' + product + ': ' + str(e))\n",
    "        if os.path.exists(cropped_product_path):\n",
    "            os.remove(cropped_product_path)\n",
    "    \n",
    "    averages = calc_average(temp_list, N_value, no_data_value)\n",
    "    \n",
    "    return max_values, averages, mask_no_data_value, geo_transform, projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_stage_in(enclosure, target_dir):\n",
    "    if not os.path.isdir(target_dir):\n",
    "        os.mkdir(target_dir)\n",
    "    return ciop.copy(urls=enclosure, target=target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_outputs(product_name, roi_name, first_date, last_date, averages, max_values, mask_no_data_value, image_format, product_count, projection, geo_transform, no_data_value):\n",
    "    filenames = []\n",
    "    filenames.append(product_name + '_' + roi_name + '_N' + str(product_count) + '_averages_' + first_date + '_' + last_date + '.tif')\n",
    "    filenames.append(product_name + '_' + roi_name + '_N' + str(product_count) + '_maxvalues_' + first_date + '_' + last_date + '.tif')\n",
    "    write_output_image(filenames[0], averages, image_format,  mask_no_data_value, projection, geo_transform, no_data_value)\n",
    "    write_output_image(filenames[1], max_values, image_format, mask_no_data_value, projection, geo_transform, no_data_value)\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_anomaly_output(anomaly, product_name, first_date, last_date, lta_start_year, lta_end_year, aggregation, mask_no_value, N_value, regionOfInterest, roi_name, projection, geo_transform, no_data_value):\n",
    "    #image_number = (datetime.strptime(last_date, '%Y-%m-%d') - datetime.strptime(first_date, '%Y-%m-%d')).days\n",
    "    filename =  product_name + 'Anomaly_' + roi_name + '_N' + str(N_value) + '_' + aggregation + '_' + first_date + '_' + last_date + '_LTA' + str(lta_start_year) + '_' + str(lta_end_year) + '.tif'\n",
    "    write_output_image(filename, anomaly, 'GTiff', mask_no_value, projection, geo_transform, no_data_value)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formatted_date(product_reference):\n",
    "    metadata = ciop.search(end_point=product_reference,\n",
    "                           params=[],\n",
    "                           output_fields='identifier,startdate',\n",
    "                           model=\"GeoTime\")\n",
    "    return metadata[0]['startdate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_properties_file(dataframe, output_name):\n",
    "    \n",
    "    title = 'Output %s' % output_name\n",
    "    first_date = get_formatted_date(dataframe.iloc[0]['self'])\n",
    "    last_date = get_formatted_date(dataframe.iloc[-1]['self'])\n",
    "    with open(output_name + '.properties', 'wb') as file:\n",
    "        file.write('title=%s\\n' % title)\n",
    "        file.write('date=%s/%s\\n' % (first_date, last_date))\n",
    "        file.write('geometry=%s' % (regionOfInterest['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, False, False, False, False, False, False, False]\n",
      "[2017]\n",
      "[1 2]\n",
      "1\n",
      "['/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_LAI-RT6_201701100000_GLOBE_PROBAV_V2.0.2.nc']\n",
      "['/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_LAI-RT6_201701100000_GLOBE_PROBAV_V2.0.2.nc', '/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_LAI-RT6_201701200000_GLOBE_PROBAV_V2.0.2.nc']\n",
      "['/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_LAI-RT6_201701100000_GLOBE_PROBAV_V2.0.2.nc', '/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_LAI-RT6_201701200000_GLOBE_PROBAV_V2.0.2.nc', '/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_LAI-RT6_201701310000_GLOBE_PROBAV_V2.0.2.nc']\n",
      "2017-01-10\n",
      "2017-01-31\n",
      "<type 'list'>\n",
      "c_gls_LAI-RT6_201701100000_GLOBE_PROBAV_V2.0.2.nc\n",
      "255.0\n",
      "c_gls_LAI-RT6_201701200000_GLOBE_PROBAV_V2.0.2.nc\n",
      "255.0\n",
      "c_gls_LAI-RT6_201701310000_GLOBE_PROBAV_V2.0.2.nc\n",
      "255.0\n",
      "['https://store.terradue.com/better-wfp-00009/_results/workflows/ec_better_wfp_01_03_04_wfp_01_03_04_1_5/run/8b6a26b2-4416-11e9-b1bf-0242ac11000f/0031195-181221095105003-oozie-oozi-W/184f169e-2050-4da8-8c6e-a7017d566334/LTA_LAI_SouthernAfrica_N3_maxvalues_1-10_1-31_2015_2017.tif']\n",
      "['https://store.terradue.com/better-wfp-00009/_results/workflows/ec_better_wfp_01_03_04_wfp_01_03_04_1_5/run/8b6a26b2-4416-11e9-b1bf-0242ac11000f/0031195-181221095105003-oozie-oozi-W/184f169e-2050-4da8-8c6e-a7017d566334/LTA_LAI_SouthernAfrica_N3_averages_1-10_1-31_2015_2017.tif']\n",
      "Calculating anomalies...\n",
      "2015\n",
      "2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:72: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015\n",
      "2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:92: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_LAI-RT6_201702100000_GLOBE_PROBAV_V2.0.2.nc']\n",
      "['/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_LAI-RT6_201702100000_GLOBE_PROBAV_V2.0.2.nc', '/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_LAI-RT6_201702200000_GLOBE_PROBAV_V2.0.2.nc']\n",
      "['/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_LAI-RT6_201702100000_GLOBE_PROBAV_V2.0.2.nc', '/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_LAI-RT6_201702200000_GLOBE_PROBAV_V2.0.2.nc', '/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_LAI-RT6_201702280000_GLOBE_PROBAV_V2.0.2.nc']\n",
      "2017-02-10\n",
      "2017-02-28\n",
      "<type 'list'>\n",
      "c_gls_LAI-RT6_201702100000_GLOBE_PROBAV_V2.0.2.nc\n",
      "255.0\n",
      "c_gls_LAI-RT6_201702200000_GLOBE_PROBAV_V2.0.2.nc\n",
      "255.0\n",
      "c_gls_LAI-RT6_201702280000_GLOBE_PROBAV_V2.0.2.nc\n",
      "255.0\n",
      "[]\n",
      "[]\n",
      "Calculating anomalies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reporter:status:2019-03-29T17:59:02.766053 [INFO   ] [user process] No Long-term Average match found for \"maxvalues\" aggregation from the period (month-day): 2-10/2-28\n",
      "2019-03-29T17:59:02.766053 [INFO   ] [user process] No Long-term Average match found for \"maxvalues\" aggregation from the period (month-day): 2-10/2-28\n",
      "reporter:status:2019-03-29T17:59:02.766617 [INFO   ] [user process] No Long-term Average match found for \"averages\" aggregation from the period (month-day): 2-10/2-28\n",
      "2019-03-29T17:59:02.766617 [INFO   ] [user process] No Long-term Average match found for \"averages\" aggregation from the period (month-day): 2-10/2-28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n",
      "1\n",
      "['/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_FAPAR-RT6_201701100000_GLOBE_PROBAV_V2.0.2.nc']\n",
      "['/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_FAPAR-RT6_201701100000_GLOBE_PROBAV_V2.0.2.nc', '/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_FAPAR-RT6_201701200000_GLOBE_PROBAV_V2.0.2.nc']\n",
      "['/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_FAPAR-RT6_201701100000_GLOBE_PROBAV_V2.0.2.nc', '/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_FAPAR-RT6_201701200000_GLOBE_PROBAV_V2.0.2.nc', '/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_FAPAR-RT6_201701310000_GLOBE_PROBAV_V2.0.2.nc']\n",
      "2017-01-10 00:00:00\n",
      "2017-01-31 00:00:00\n",
      "<type 'list'>\n",
      "c_gls_FAPAR-RT6_201701100000_GLOBE_PROBAV_V2.0.2.nc\n",
      "255.0\n",
      "c_gls_FAPAR-RT6_201701200000_GLOBE_PROBAV_V2.0.2.nc\n",
      "255.0\n",
      "c_gls_FAPAR-RT6_201701310000_GLOBE_PROBAV_V2.0.2.nc\n",
      "255.0\n",
      "['https://store.terradue.com/better-wfp-00009/_results/workflows/ec_better_wfp_01_03_04_wfp_01_03_04_1_5/run/8b6a26b2-4416-11e9-b1bf-0242ac11000f/0031195-181221095105003-oozie-oozi-W/184f169e-2050-4da8-8c6e-a7017d566334/LTA_FAPAR_SouthernAfrica_N3_maxvalues_1-10_1-31_2015_2017.tif']\n",
      "['https://store.terradue.com/better-wfp-00009/_results/workflows/ec_better_wfp_01_03_04_wfp_01_03_04_1_5/run/8b6a26b2-4416-11e9-b1bf-0242ac11000f/0031195-181221095105003-oozie-oozi-W/184f169e-2050-4da8-8c6e-a7017d566334/LTA_FAPAR_SouthernAfrica_N3_averages_1-10_1-31_2015_2017.tif']\n",
      "Calculating anomalies...\n",
      "2015\n",
      "2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:236: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015\n",
      "2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:257: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_FAPAR-RT6_201702100000_GLOBE_PROBAV_V2.0.2.nc']\n",
      "['/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_FAPAR-RT6_201702100000_GLOBE_PROBAV_V2.0.2.nc', '/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_FAPAR-RT6_201702200000_GLOBE_PROBAV_V2.0.2.nc']\n",
      "['/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_FAPAR-RT6_201702100000_GLOBE_PROBAV_V2.0.2.nc', '/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_FAPAR-RT6_201702200000_GLOBE_PROBAV_V2.0.2.nc', '/workspace/wfp-01-03-01/src/main/app-resources/notebook/libexec/tmp_data/c_gls_FAPAR-RT6_201702280000_GLOBE_PROBAV_V2.0.2.nc']\n",
      "2017-02-10 00:00:00\n",
      "2017-02-28 00:00:00\n",
      "<type 'list'>\n",
      "c_gls_FAPAR-RT6_201702100000_GLOBE_PROBAV_V2.0.2.nc\n",
      "255.0\n",
      "c_gls_FAPAR-RT6_201702200000_GLOBE_PROBAV_V2.0.2.nc\n",
      "255.0\n",
      "c_gls_FAPAR-RT6_201702280000_GLOBE_PROBAV_V2.0.2.nc\n",
      "255.0\n",
      "[]\n",
      "[]\n",
      "Calculating anomalies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reporter:status:2019-03-29T18:01:54.018399 [INFO   ] [user process] No Long-term Average match found for \"maxvalues\" aggregation from the period (month-day): 2-10/2-28\n",
      "2019-03-29T18:01:54.018399 [INFO   ] [user process] No Long-term Average match found for \"maxvalues\" aggregation from the period (month-day): 2-10/2-28\n",
      "reporter:status:2019-03-29T18:01:54.018974 [INFO   ] [user process] No Long-term Average match found for \"averages\" aggregation from the period (month-day): 2-10/2-28\n",
      "2019-03-29T18:01:54.018974 [INFO   ] [user process] No Long-term Average match found for \"averages\" aggregation from the period (month-day): 2-10/2-28\n"
     ]
    }
   ],
   "source": [
    "nlist = [N_1['value'], N_3['value'], N_6['value'], N_9['value'], N_12['value'], N_15['value'], N_18['value'], N_27['value'], N_36['value']]\n",
    "nvalue = [1, 3, 6, 9, 12, 15, 18, 27, 36]\n",
    "#L = len(gpd_final.index.values)\n",
    "nlist = [n=='True' for n in nlist]\n",
    "print(nlist)\n",
    "if gpd_lai_final is not None:\n",
    "    lai_product_years = gpd_lai_final['startdate'].dt.year.unique()\n",
    "else:\n",
    "    lai_product_years = []\n",
    "if gpd_fapar_final is not None:\n",
    "    fapar_product_years = gpd_fapar_final['startdate'].dt.year.unique()\n",
    "else:\n",
    "    fapar_product_years = []\n",
    "product_years = list(lai_product_years) + list(set(fapar_product_years) - set(lai_product_years))\n",
    "print(product_years)\n",
    "for _year in product_years:\n",
    "    #products_data = gpd_final[(gpd_final['startdate'].dt.year == _year)]\n",
    "    #lai_products = products_data[(products_data['identifier'].str.contains('lai'))]\n",
    "    #fapar_products = products_data[(products_data['identifier'].str.contains('fapar'))]\n",
    "    lai_products = gpd_lai_final[(gpd_lai_final['startdate'].dt.year == _year)]\n",
    "    fapar_products = gpd_fapar_final[(gpd_fapar_final['startdate'].dt.year == _year)]\n",
    "    if not lai_products.empty:\n",
    "        products_data = lai_products\n",
    "        L = len(products_data.index.values)\n",
    "        months_of_products = products_data['startdate'].dt.month.unique()\n",
    "        print(months_of_products)\n",
    "        for n in [index for index, value in enumerate(nlist) if value==True]:\n",
    "            N = nvalue[n]\n",
    "            n_months = N/3\n",
    "            if N == 1:\n",
    "                pass\n",
    "            else: \n",
    "                if N == 3:\n",
    "                    for month_idx in months_of_products:\n",
    "                        print(month_idx)\n",
    "                        start_date = products_data[(products_data['startdate'].dt.month == month_idx)].iloc[0]['startdate']\n",
    "                        end_date = products_data[(products_data['startdate'].dt.month == month_idx)].iloc[-1]['startdate']\n",
    "                        interval_gpd = products_data[(products_data['startdate'] >= start_date) & (products_data['startdate'] <= end_date)]\n",
    "                        interval_products = interval_gpd['enclosure'].tolist()\n",
    "                        \n",
    "                        file_list = []\n",
    "                        for enclosure in interval_products:\n",
    "                            filepath = product_stage_in(enclosure, 'tmp_data')\n",
    "                            file_list.append(filepath)\n",
    "                            print(file_list)\n",
    "                        if file_list:\n",
    "                            first_date = start_date.strftime('%Y-%m-%d')\n",
    "                            last_date = end_date.strftime('%Y-%m-%d')\n",
    "                            print(first_date)\n",
    "                            print(last_date)\n",
    "                            max_values, averages, no_value, geo_transform, projection = calc_aggregations(file_list, N, region_of_interest, 'LAI')\n",
    "                        \n",
    "                            \n",
    "                            LTA_max_values = get_lta_from_dataframe(lta_data, 'LAI', 'maxvalues', nameOfRegion, start_date.day, end_date.day, start_date.month, end_date.month, region_of_interest)\n",
    "                            LTA_averages = get_lta_from_dataframe(lta_data, 'LAI', 'averages', nameOfRegion, start_date.day, end_date.day, start_date.month, end_date.month, region_of_interest)\n",
    "                            print(LTA_max_values)\n",
    "                            print(LTA_averages)\n",
    "                            \n",
    "                            anomaly_files = []\n",
    "                            print('Calculating anomalies...')\n",
    "                            if len(LTA_max_values) > 0:\n",
    "                                LTA_max_values = LTA_max_values[0]\n",
    "                                start_year = re.findall('\\d{4}', os.path.basename(LTA_max_values))[0]\n",
    "                                end_year = re.findall('\\d{4}', os.path.basename(LTA_max_values))[1]\n",
    "                                print(start_year)\n",
    "                                print(end_year)\n",
    "                                filepath = 'tmp_data/' + os.path.basename(LTA_max_values)\n",
    "                                status = get_product(LTA_max_values, filepath)\n",
    "                                if status == 200:\n",
    "                                    filepath = [filepath]\n",
    "                                    LTA_max_values = get_matrix_list(filepath)[0]\n",
    "                                    anomaly_max_values = np.divide(max_values, LTA_max_values)\n",
    "                                    filename = write_anomaly_output(anomaly_max_values, 'LAI', first_date, last_date, start_year, end_year, 'maxvalues', no_value, N, region_of_interest, nameOfRegion, projection, geo_transform, -999.0)\n",
    "                                    anomaly_files.append(filename)\n",
    "                                    os.remove(filepath[0])\n",
    "                            else:\n",
    "                                period_str = str(start_date.month) + '-' + str(start_date.day) + '/' + str(end_date.month) + '-' + str(end_date.day)\n",
    "                                message = 'No Long-term Average match found for \"maxvalues\" aggregation from the period (month-day): ' + period_str \n",
    "                                ciop.log('INFO', message)\n",
    "                                \n",
    "                            if len(LTA_averages) > 0:\n",
    "                                LTA_averages = LTA_averages[0]\n",
    "                                start_year = re.findall('\\d{4}', os.path.basename(LTA_averages))[0]\n",
    "                                end_year = re.findall('\\d{4}', os.path.basename(LTA_averages))[1]\n",
    "                                print(start_year)\n",
    "                                print(end_year)\n",
    "                                filepath = 'tmp_data/' + os.path.basename(LTA_averages)\n",
    "                                status = get_product(LTA_averages, filepath)\n",
    "                                if status == 200:\n",
    "                                    filepath = [filepath]\n",
    "                                    LTA_averages = get_matrix_list(filepath)[0]\n",
    "                                    anomaly_averages = np.divide(averages, LTA_averages)\n",
    "                                    filename = write_anomaly_output(anomaly_averages, 'LAI', first_date, last_date, start_year, end_year, 'averages', no_value, N, region_of_interest, nameOfRegion, projection, geo_transform, -999.0)\n",
    "                                    anomaly_files.append(filename)\n",
    "                                    os.remove(filepath[0])\n",
    "                                \n",
    "                            else:\n",
    "                                period_str = str(start_date.month) + '-' + str(start_date.day) + '/' + str(end_date.month) + '-' + str(end_date.day)\n",
    "                                message = 'No Long-term Average match found for \"averages\" aggregation from the period (month-day): ' + period_str \n",
    "                                ciop.log('INFO', message)\n",
    "                            \n",
    "                        \n",
    "                            filenames = write_outputs('LAI', nameOfRegion, first_date, last_date, averages, max_values, no_value, 'GTiff', N, projection, geo_transform, -999.0)\n",
    "                            filenames = filenames + anomaly_files\n",
    "                            for output_name in filenames:\n",
    "                                write_properties_file(interval_gpd, output_name)\n",
    "                            for tmp_file in file_list:\n",
    "                                os.remove(tmp_file)\n",
    "                        else:\n",
    "                            print('ERROR: No LAI product files found.')\n",
    "                else:\n",
    "                    for month_idx in range(months_of_products[0], months_of_products[-1], n_months):\n",
    "                        start_date = products_data[(products_data['startdate'].dt.month == month_idx)].iloc[0]['startdate']\n",
    "                        end_date = products_data[(products_data['startdate'].dt.month == month_idx + n_months-1)].iloc[-1]['startdate']\n",
    "                        interval_gpd = products_data[(products_data['startdate'] >= start_date) & (products_data['startdate'] <= end_date)]\n",
    "                        interval_products = interval_gpd['enclosure'].tolist()\n",
    "                        \n",
    "                        file_list = []\n",
    "                        for enclosure in interval_products:\n",
    "                            filepath = product_stage_in(enclosure, 'tmp_data')\n",
    "                            file_list.append(filepath)\n",
    "                            print(file_list)\n",
    "                        if file_list:\n",
    "                            first_date = start_date.strftime('%Y-%m-%d')\n",
    "                            last_date = end_date.strftime('%Y-%m-%d')\n",
    "                            print(start_date)\n",
    "                            print(end_date)\n",
    "                            max_values, averages, no_value, geo_transform, projection = calc_aggregations(file_list, N, region_of_interest, 'LAI')\n",
    "                        \n",
    "                            LTA_max_values = get_lta_from_dataframe(lta_data, 'LAI', 'maxvalues', nameOfRegion, start_date.day, end_date.day, start_date.month, end_date.month, region_of_interest)\n",
    "                            LTA_averages = get_lta_from_dataframe(lta_data, 'LAI', 'averages', nameOfRegion, start_date.day, end_date.day, start_date.month, end_date.month, region_of_interest)\n",
    "                            print(LTA_max_values)\n",
    "                            print(LTA_averages)\n",
    "                            \n",
    "                            anomaly_files = []\n",
    "                            print('Calculating anomalies...')\n",
    "                            if len(LTA_max_values) > 0:\n",
    "                                LTA_max_values = LTA_max_values[0]\n",
    "                                start_year = re.findall('\\d{4}', os.path.basename(LTA_max_values))[0]\n",
    "                                end_year = re.findall('\\d{4}', os.path.basename(LTA_max_values))[1]\n",
    "                                print(start_year)\n",
    "                                print(end_year)\n",
    "                                filepath = 'tmp_data/' + os.path.basename(LTA_max_values)\n",
    "                                status = get_product(LTA_max_values, filepath)\n",
    "                                if status == 200:\n",
    "                                    filepath = [filepath]\n",
    "                                    LTA_max_values = get_matrix_list(filepath)[0]\n",
    "                                    anomaly_max_values = np.divide(max_values, LTA_max_values)\n",
    "                                    filename = write_anomaly_output(anomaly_max_values, 'LAI', first_date, last_date, start_year, end_year, 'maxvalues', no_value, N, region_of_interest, nameOfRegion, projection, geo_transform, -999.0)\n",
    "                                    anomaly_files.append(filename)\n",
    "                                    os.remove(filepath[0])\n",
    "                            \n",
    "                            else:\n",
    "                                period_str = str(start_date.month) + '-' + str(start_date.day) + '/' + str(end_date.month) + '-' + str(end_date.day)\n",
    "                                message = 'No Long-term Average match found for \"maxvalues\" aggregation from the period (month-day): ' + period_str \n",
    "                                ciop.log('INFO', message)\n",
    "                            \n",
    "                            if len(LTA_averages) > 0:\n",
    "                                LTA_averages = LTA_averages[0]\n",
    "                                start_year = re.findall('\\d{4}', os.path.basename(LTA_averages))[0]\n",
    "                                end_year = re.findall('\\d{4}', os.path.basename(LTA_averages))[1]\n",
    "                                filepath = 'tmp_data/' + os.path.basename(LTA_averages)\n",
    "                                status = get_product(LTA_averages, filepath)\n",
    "                                if status == 200:\n",
    "                                    filepath = [filepath]\n",
    "                                    LTA_averages = get_matrix_list(filepath)[0]\n",
    "                                    \n",
    "                                    anomaly_averages = np.divide(averages, LTA_averages)\n",
    "                                    filename = write_anomaly_output(anomaly_averages, 'LAI', first_date, last_date, start_date.year, end_date.year, 'averages', no_value, N, region_of_interest, nameOfRegion, projection, geo_transform, -999.0)\n",
    "                                    anomaly_files.append(filename)\n",
    "                                    os.remove(filepath[0])\n",
    "                            \n",
    "                            else:\n",
    "                                period_str = str(start_date.month) + '-' + str(start_date.day) + '/' + str(end_date.month) + '-' + str(end_date.day)\n",
    "                                message = 'No Long-term Average match found for \"averages\" aggregation from the period (month-day): ' + period_str \n",
    "                                ciop.log('INFO', message)\n",
    "                        \n",
    "                            filenames = write_outputs('LAI', nameOfRegion, first_date, last_date, averages, max_values, no_value, 'GTiff', N, projection, geo_transform, -999.0)\n",
    "                            filenames = filenames + anomaly_files\n",
    "                            for output_name in filenames:\n",
    "                                write_properties_file(interval_gpd, output_name)\n",
    "                            for tmp_file in file_list:\n",
    "                                os.remove(tmp_file)\n",
    "                        else:\n",
    "                            print('ERROR: No LAI product files found.')\n",
    "    \n",
    "    if not fapar_products.empty:\n",
    "        products_data = fapar_products\n",
    "        L = len(products_data.index.values)\n",
    "        months_of_products = products_data['startdate'].dt.month.unique()\n",
    "        print(months_of_products)\n",
    "        for n in [index for index, value in enumerate(nlist) if value==True]:\n",
    "            N = nvalue[n]\n",
    "            n_months = N/3\n",
    "            if N == 1:\n",
    "                pass\n",
    "            else: \n",
    "                if N == 3:\n",
    "                    for month_idx in months_of_products:\n",
    "                        print(month_idx)\n",
    "                        start_date = products_data[(products_data['startdate'].dt.month == month_idx)].iloc[0]['startdate']\n",
    "                        end_date = products_data[(products_data['startdate'].dt.month == month_idx)].iloc[-1]['startdate']\n",
    "                        interval_gpd = products_data[(products_data['startdate'] >= start_date) & (products_data['startdate'] <= end_date)]\n",
    "                        interval_products = interval_gpd['enclosure'].tolist()\n",
    "                        \n",
    "                        file_list = []\n",
    "                        for enclosure in interval_products:\n",
    "                            filepath = product_stage_in(enclosure, 'tmp_data')\n",
    "                            file_list.append(filepath)\n",
    "                            print(file_list)\n",
    "                        if file_list:\n",
    "                            first_date = start_date.strftime('%Y-%m-%d')\n",
    "                            last_date = end_date.strftime('%Y-%m-%d')\n",
    "                            print(start_date)\n",
    "                            print(end_date)\n",
    "                            max_values, averages, no_value, geo_transform, projection = calc_aggregations(file_list, N, region_of_interest, 'FAPAR')\n",
    "                        \n",
    "                            LTA_max_values = get_lta_from_dataframe(lta_data, 'FAPAR', 'maxvalues', nameOfRegion, start_date.day, end_date.day, start_date.month, end_date.month, region_of_interest)\n",
    "                            LTA_averages = get_lta_from_dataframe(lta_data, 'FAPAR', 'averages', nameOfRegion, start_date.day, end_date.day, start_date.month, end_date.month, region_of_interest)\n",
    "                            print(LTA_max_values)\n",
    "                            print(LTA_averages)\n",
    "                            \n",
    "                            anomaly_files = []\n",
    "                            print('Calculating anomalies...')\n",
    "                            if len(LTA_max_values) > 0:\n",
    "                                LTA_max_values = LTA_max_values[0]\n",
    "                                start_year = re.findall('\\d{4}', os.path.basename(LTA_max_values))[0]\n",
    "                                end_year = re.findall('\\d{4}', os.path.basename(LTA_max_values))[1]\n",
    "                                print(start_year)\n",
    "                                print(end_year)\n",
    "                                filepath = 'tmp_data/' + os.path.basename(LTA_max_values)\n",
    "                                status = get_product(LTA_max_values, filepath)\n",
    "                                if status == 200:\n",
    "                                    filepath = [filepath]\n",
    "                                    LTA_max_values = get_matrix_list(filepath)[0]\n",
    "                                    anomaly_max_values = np.divide(max_values, LTA_max_values)\n",
    "                                    filename = write_anomaly_output(anomaly_max_values, 'FAPAR', first_date, last_date, start_year, end_year, 'maxvalues', no_value, N, region_of_interest, nameOfRegion, projection, geo_transform, -999.0)\n",
    "                                    anomaly_files.append(filename)\n",
    "                                    os.remove(filepath[0])\n",
    "                                    \n",
    "                            else:\n",
    "                                period_str = str(start_date.month) + '-' + str(start_date.day) + '/' + str(end_date.month) + '-' + str(end_date.day)\n",
    "                                message = 'No Long-term Average match found for \"maxvalues\" aggregation from the period (month-day): ' + period_str \n",
    "                                ciop.log('INFO', message)\n",
    "                                \n",
    "                            if len(LTA_averages) > 0:\n",
    "                                LTA_averages = LTA_averages[0]\n",
    "                                start_year = re.findall('\\d{4}', os.path.basename(LTA_averages))[0]\n",
    "                                end_year = re.findall('\\d{4}', os.path.basename(LTA_averages))[1]\n",
    "                                print(start_year)\n",
    "                                print(end_year)\n",
    "                                filepath = 'tmp_data/' + os.path.basename(LTA_averages)\n",
    "                                status = get_product(LTA_averages, filepath)\n",
    "                                if status == 200:\n",
    "                                    filepath = [filepath]\n",
    "                                    LTA_averages = get_matrix_list(filepath)[0]\n",
    "                                    anomaly_averages = np.divide(averages, LTA_averages)\n",
    "                                    filename = write_anomaly_output(anomaly_averages, 'FAPAR', first_date, last_date, start_year, end_year, 'averages', no_value, N, region_of_interest, nameOfRegion, projection, geo_transform, -999.0)\n",
    "                                    anomaly_files.append(filename)\n",
    "                                    os.remove(filepath[0])\n",
    "                            \n",
    "                            else:\n",
    "                                period_str = str(start_date.month) + '-' + str(start_date.day) + '/' + str(end_date.month) + '-' + str(end_date.day)\n",
    "                                message = 'No Long-term Average match found for \"averages\" aggregation from the period (month-day): ' + period_str \n",
    "                                ciop.log('INFO', message)\n",
    "                            \n",
    "                        \n",
    "                            filenames = write_outputs('FAPAR', nameOfRegion, first_date, last_date, averages, max_values, no_value, 'GTiff', N, projection, geo_transform, -999.0)\n",
    "                            filenames = filenames + anomaly_files\n",
    "                            for output_name in filenames:\n",
    "                                write_properties_file(interval_gpd, output_name)\n",
    "                            for tmp_file in file_list:\n",
    "                                os.remove(tmp_file)\n",
    "                        else:\n",
    "                            print('ERROR: No fAPAR product files found.')\n",
    "                    \n",
    "                else:\n",
    "                    for month_idx in range(months_of_products[0], months_of_products[-1], n_months):\n",
    "                        print(month_idx)\n",
    "                        start_date = products_data[(products_data['startdate'].dt.month == month_idx)].iloc[0]['startdate']\n",
    "                        end_date = products_data[(products_data['startdate'].dt.month == month_idx + n_months-1)].iloc[-1]['startdate']\n",
    "                        interval_gpd = products_data[(products_data['startdate'] >= start_date) & (products_data['startdate'] <= end_date)]\n",
    "                        interval_products = interval_gpd['enclosure'].tolist()\n",
    "                        \n",
    "                        file_list = []\n",
    "                        for enclosure in interval_products:\n",
    "                            filepath = product_stage_in(enclosure, 'tmp_data')\n",
    "                            file_list.append(filepath)\n",
    "                            print(file_list)\n",
    "                        if file_list:\n",
    "                            first_date = start_date.strftime('%Y-%m-%d')\n",
    "                            last_date = end_date.strftime('%Y-%m-%d')\n",
    "                            print(start_date)\n",
    "                            print(end_date)\n",
    "                            max_values, averages, no_value, geo_transform, projection = calc_aggregations(file_list, N, region_of_interest, 'FAPAR')\n",
    "                        \n",
    "                            LTA_max_values = get_lta_from_dataframe(lta_data, 'FAPAR', 'maxvalues', nameOfRegion, start_date.day, end_date.day, start_date.month, end_date.month, region_of_interest)\n",
    "                            LTA_averages = get_lta_from_dataframe(lta_data, 'FAPAR', 'averages', nameOfRegion, start_date.day, end_date.day, start_date.month, end_date.month, region_of_interest)\n",
    "                            print(LTA_max_values)\n",
    "                            print(LTA_averages)\n",
    "                            \n",
    "                            anomaly_files = []\n",
    "                            print('Calculating anomalies...')\n",
    "                            if len(LTA_max_values) > 0:\n",
    "                                LTA_max_values = LTA_max_values[0]\n",
    "                                start_year = re.findall('\\d{4}', os.path.basename(LTA_max_values))[0]\n",
    "                                end_year = re.findall('\\d{4}', os.path.basename(LTA_max_values))[1]\n",
    "                                print(start_year)\n",
    "                                print(end_year)\n",
    "                                filepath = 'tmp_data/' + os.path.basename(LTA_max_values)\n",
    "                                status = get_product(LTA_max_values, filepath)\n",
    "                                if status == 200:\n",
    "                                    filepath = [filepath]\n",
    "                                    LTA_max_values = get_matrix_list(filepath)[0]\n",
    "                                    anomaly_max_values = np.divide(max_values, LTA_max_values)\n",
    "                                    filename = write_anomaly_output(anomaly_max_values, 'FAPAR', first_date, last_date, start_year, end_year, 'maxvalues', no_value, N, region_of_interest, nameOfRegion, projection, geo_transform, -999.0)\n",
    "                                    anomaly_files.append(filename)\n",
    "                                    os.remove(filepath[0])\n",
    "                                    \n",
    "                            else:\n",
    "                                period_str = str(start_date.month) + '-' + str(start_date.day) + '/' + str(end_date.month) + '-' + str(end_date.day)\n",
    "                                message = 'No Long-term Average match found for \"maxvalues\" aggregation from the period (month-day): ' + period_str \n",
    "                                ciop.log('INFO', message)\n",
    "                                \n",
    "                            \n",
    "                            if len(LTA_averages) > 0:\n",
    "                                LTA_averages = LTA_averages[0]\n",
    "                                start_year = re.findall('\\d{4}', os.path.basename(LTA_averages))[0]\n",
    "                                end_year = re.findall('\\d{4}', os.path.basename(LTA_averages))[1]\n",
    "                                print(start_year)\n",
    "                                print(end_year)\n",
    "                                filepath = 'tmp_data/' + os.path.basename(LTA_averages)\n",
    "                                status = get_product(LTA_averages, filepath)\n",
    "                                if status == 200:\n",
    "                                    filepath = [filepath]\n",
    "                                    LTA_averages = get_matrix_list(filepath)[0]\n",
    "                                    anomaly_averages = np.divide(averages, LTA_averages)\n",
    "                                    filename = write_anomaly_output(anomaly_averages, 'FAPAR', first_date, last_date, start_year, end_year, 'averages', no_value, N, region_of_interest, nameOfRegion, projection, geo_transform, -999.0)\n",
    "                                    anomaly_files.append(filename)\n",
    "                                    os.remove(filepath[0])\n",
    "                            \n",
    "                            else:\n",
    "                                period_str = str(start_date.month) + '-' + str(start_date.day) + '/' + str(end_date.month) + '-' + str(end_date.day)\n",
    "                                message = 'No Long-term Average match found for \"averages\" aggregation from the period (month-day): ' + period_str \n",
    "                                ciop.log('INFO', message)\n",
    "                        \n",
    "                            filenames = write_outputs('FAPAR', nameOfRegion, first_date, last_date, averages, max_values, no_value, 'GTiff', N, projection, geo_transform, -999.0)\n",
    "                            filenames = filenames + anomaly_files\n",
    "                            for output_name in filenames:\n",
    "                                write_properties_file(interval_gpd, output_name)\n",
    "                            for tmp_file in file_list:\n",
    "                                os.remove(tmp_file)\n",
    "                        else:\n",
    "                            print('ERROR: No fAPAR product files found.')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from matplotlib import pyplot\\n%matplotlib inline\\n\\n\\nfig0 = pyplot.figure()\\nax0 = fig0.add_subplot(111)\\ncax0 = ax0.matshow(max_values)\\nfig0.colorbar(cax0)\\n\\nfig1 = pyplot.figure()\\nax1 = fig1.add_subplot(111)\\ncax1 = ax1.matshow(averages)\\nfig1.colorbar(cax1)\\n\\n\\nfig = pyplot.figure()\\nax = fig.add_subplot(111)\\ncax = ax.matshow(no_value)\\nfig.colorbar(cax)\\n\\npyplot.show()\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "fig0 = pyplot.figure()\n",
    "ax0 = fig0.add_subplot(111)\n",
    "cax0 = ax0.matshow(max_values)\n",
    "fig0.colorbar(cax0)\n",
    "\n",
    "fig1 = pyplot.figure()\n",
    "ax1 = fig1.add_subplot(111)\n",
    "cax1 = ax1.matshow(averages)\n",
    "fig1.colorbar(cax1)\n",
    "\n",
    "\n",
    "fig = pyplot.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(no_value)\n",
    "fig.colorbar(cax)\n",
    "\n",
    "pyplot.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"license\">License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work is licenced under a [Attribution-ShareAlike 4.0 International License (CC BY-SA 4.0)](http://creativecommons.org/licenses/by-sa/4.0/) \n",
    "\n",
    "YOU ARE FREE TO:\n",
    "\n",
    "* Share - copy and redistribute the material in any medium or format.\n",
    "* Adapt - remix, transform, and built upon the material for any purpose, even commercially.\n",
    "\n",
    "UNDER THE FOLLOWING TERMS:\n",
    "\n",
    "* Attribution - You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n",
    "* ShareAlike - If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
